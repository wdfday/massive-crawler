package polygon

import (
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"strconv"
	"time"

	"us-data/saver"
)

const (
	// Limit tá»‘i Ä‘a 50k results má»—i request
	maxLimit = 50000

	// Sá»‘ ngÃ y tá»‘i Ä‘a cho má»—i request aggregates 1-minute
	// TÃ­nh toÃ¡n: 50,000 bars / ~960 phÃºt/ngÃ y (extended hours) â‰ˆ 52 ngÃ y
	// Äá»ƒ an toÃ n, Ä‘áº·t 50 ngÃ y: 50 * 960 = 48,000 < 50,000 (cáº­n tá»‘i Ä‘a)
	maxDaysPerRequest = 50
)

// AggregatesResponse lÃ  response tá»« Polygon API vá»›i next_url
type AggregatesResponse struct {
	Ticker       string   `json:"ticker"`
	QueryCount   int      `json:"queryCount"`
	ResultsCount int      `json:"resultsCount"`
	Adjusted     bool     `json:"adjusted"`
	Results      []BarRaw `json:"results"` // Parse vá»›i BarRaw trÆ°á»›c, sau Ä‘Ã³ convert
	Status       string   `json:"status"`
	RequestID    string   `json:"request_id"`
	Count        int      `json:"count"`
	NextURL      string   `json:"next_url,omitempty"`
}

// FlexibleInt64 lÃ  type Ä‘á»ƒ parse cáº£ int vÃ  float (scientific notation) thÃ nh int64
type FlexibleInt64 int64

// UnmarshalJSON custom unmarshaler Ä‘á»ƒ parse cáº£ int vÃ  float
func (f *FlexibleInt64) UnmarshalJSON(data []byte) error {
	// Thá»­ parse nhÆ° string trÆ°á»›c Ä‘á»ƒ handle scientific notation
	var str string
	if err := json.Unmarshal(data, &str); err == nil {
		// Parse tá»« string
		val, err := strconv.ParseFloat(str, 64)
		if err != nil {
			return err
		}
		*f = FlexibleInt64(int64(val))
		return nil
	}

	// Thá»­ parse nhÆ° float64
	var floatVal float64
	if err := json.Unmarshal(data, &floatVal); err == nil {
		*f = FlexibleInt64(int64(floatVal))
		return nil
	}

	// Thá»­ parse nhÆ° int64
	var intVal int64
	if err := json.Unmarshal(data, &intVal); err == nil {
		*f = FlexibleInt64(intVal)
		return nil
	}

	return fmt.Errorf("khÃ´ng thá»ƒ parse thÃ nh int64: %s", string(data))
}

// Int64 tráº£ vá» giÃ¡ trá»‹ int64
func (f FlexibleInt64) Int64() int64 {
	return int64(f)
}

// BarRaw lÃ  struct táº¡m Ä‘á»ƒ parse JSON vá»›i FlexibleInt64 cho Volume vÃ  Transactions
type BarRaw struct {
	Timestamp    int64         `json:"t"` // Unix timestamp in milliseconds
	Open         float64       `json:"o"`
	High         float64       `json:"h"`
	Low          float64       `json:"l"`
	Close        float64       `json:"c"`
	Volume       FlexibleInt64 `json:"v"` // Parse vá»›i FlexibleInt64 Ä‘á»ƒ handle cáº£ int vÃ  float
	VWAP         float64       `json:"vw,omitempty"`
	Transactions FlexibleInt64 `json:"n,omitempty"` // Parse vá»›i FlexibleInt64
}

// ToBar convert BarRaw sang Bar
func (br BarRaw) ToBar() Bar {
	return Bar{
		Timestamp:    br.Timestamp,
		Open:         br.Open,
		High:         br.High,
		Low:          br.Low,
		Close:        br.Close,
		Volume:       br.Volume.Int64(), // Convert FlexibleInt64 -> int64
		VWAP:         br.VWAP,
		Transactions: br.Transactions.Int64(), // Convert FlexibleInt64 -> int64
	}
}

// TickersResponse lÃ  response tá»« Tickers API vá»›i next_url
type TickersResponse struct {
	Status    string   `json:"status"`
	RequestID string   `json:"request_id,omitempty"`
	Count     int      `json:"count"`
	NextURL   string   `json:"next_url,omitempty"`
	Results   []Ticker `json:"results"`
}

// Ticker Ä‘áº¡i diá»‡n cho má»™t ticker trong danh sÃ¡ch
type Ticker struct {
	Ticker string `json:"ticker"`
	Name   string `json:"name"`
	Market string `json:"market"`
	Active bool   `json:"active"`
}

// RateLimiter interface cho rate limiting
type RateLimiter interface {
	WaitForKey(apiKey string) error
	Wait() error
	Close() error
	SaveNextURL(ticker string, nextURL string) error
	GetNextURL(ticker string) (string, error)
	DeleteNextURL(ticker string) error
}

// RateLimitedTransport lÃ  custom HTTP transport Ä‘á»ƒ tÃ­ch há»£p rate limiter
type RateLimitedTransport struct {
	baseTransport http.RoundTripper
	rateLimiter   RateLimiter
	getAPIKey     func() (string, error)
}

// RoundTrip thá»±c hiá»‡n request vá»›i rate limiting
func (t *RateLimitedTransport) RoundTrip(req *http.Request) (*http.Response, error) {
	// Láº¥y API key
	apiKey, err := t.getAPIKey()
	if err != nil {
		return nil, fmt.Errorf("lá»—i khi láº¥y API key: %w", err)
	}

	// Äá»£i rate limiter TRÆ¯á»šC KHI gá»i API
	if err := t.rateLimiter.WaitForKey(apiKey); err != nil {
		return nil, fmt.Errorf("lá»—i khi Ä‘á»£i rate limiter: %w", err)
	}

	// Set Connection: close Ä‘á»ƒ kill connection reuse vÃ  trÃ¡nh timeout
	// CÃ³ thá»ƒ connection reuse gÃ¢y ra váº¥n Ä‘á» timeout tá»« chunk 2 trá»Ÿ Ä‘i
	req.Header.Set("Connection", "close")

	// Forward request Ä‘áº¿n base transport
	resp, err := t.baseTransport.RoundTrip(req)
	if err != nil {
		return nil, err
	}

	return resp, nil
}

// Crawler quáº£n lÃ½ viá»‡c crawl dá»¯ liá»‡u tá»« Massive API
type Crawler struct {
	apiKey        string      // API key hiá»‡n táº¡i (cho compatibility)
	keyPool       *APIKeyPool // Pool cá»§a nhiá»u API keys
	rateLimiter   RateLimiter // Rate limiter interface
	httpClient    *http.Client
	useKeyPool    bool              // Flag Ä‘á»ƒ biáº¿t cÃ³ dÃ¹ng key pool khÃ´ng
	SavePacketDir string            // ThÆ° má»¥c lÆ°u packet: {SavePacketDir}/{ticker}/{ticker}_{from}_to_{to}.{ext}
	PacketSaver   saver.PacketSaver // Inject tá»« ngoÃ i â€” DIP. Náº¿u nil thÃ¬ khÃ´ng lÆ°u packet.
}

// NewCrawler táº¡o má»™t crawler má»›i vá»›i in-memory rate limiting (single API key)
func NewCrawler(apiKey string) (*Crawler, error) {
	// Táº¡o in-memory rate limiter
	rateLimiter := NewInMemoryRateLimiter()

	// Táº¡o custom HTTP transport vá»›i timeout dÃ i vÃ  rate limiter
	// SDK (RESTY) cÃ³ thá»ƒ cÃ³ timeout riÃªng, nÃªn cáº§n transport vá»›i timeout dÃ i
	// Disable connection reuse Ä‘á»ƒ trÃ¡nh timeout tá»« chunk 2 trá»Ÿ Ä‘i
	baseTransport := &http.Transport{
		ResponseHeaderTimeout: 10 * time.Minute, // Timeout cho response headers
		IdleConnTimeout:       0,                // Disable connection reuse (0 = khÃ´ng reuse)
		TLSHandshakeTimeout:   10 * time.Second,
		DisableKeepAlives:     true, // Disable keep-alive Ä‘á»ƒ force close connection
		MaxIdleConns:          0,    // KhÃ´ng giá»¯ idle connections
		MaxIdleConnsPerHost:   0,    // KhÃ´ng giá»¯ idle connections per host
	}

	transport := &RateLimitedTransport{
		baseTransport: baseTransport,
		rateLimiter:   rateLimiter,
		getAPIKey: func() (string, error) {
			return apiKey, nil
		},
	}

	httpClient := &http.Client{
		Transport: transport,
		Timeout:   10 * time.Minute, // TÄƒng timeout lÃªn 10 phÃºt cho request lá»›n (50k bars)
	}

	return &Crawler{
		apiKey:      apiKey,
		rateLimiter: rateLimiter,
		httpClient:  httpClient,
		useKeyPool:  false,
	}, nil
}

// NewCrawlerWithKeyPool táº¡o crawler má»›i vá»›i nhiá»u API keys
func NewCrawlerWithKeyPool(apiKeys []string, strategy KeySelectionStrategy) (*Crawler, error) {
	if len(apiKeys) == 0 {
		return nil, fmt.Errorf("cáº§n Ã­t nháº¥t má»™t API key")
	}

	// Táº¡o key pool vá»›i in-memory rate limiter
	keyPool, err := NewAPIKeyPool(apiKeys, strategy)
	if err != nil {
		return nil, fmt.Errorf("lá»—i khi táº¡o key pool: %w", err)
	}

	rateLimiter := keyPool.GetRateLimiter()

	// Táº¡o custom HTTP transport vá»›i timeout dÃ i vÃ  rate limiter
	// SDK (RESTY) cÃ³ thá»ƒ cÃ³ timeout riÃªng, nÃªn cáº§n transport vá»›i timeout dÃ i
	// Disable connection reuse Ä‘á»ƒ trÃ¡nh timeout tá»« chunk 2 trá»Ÿ Ä‘i
	baseTransport := &http.Transport{
		ResponseHeaderTimeout: 10 * time.Minute, // Timeout cho response headers
		IdleConnTimeout:       0,                // Disable connection reuse (0 = khÃ´ng reuse)
		TLSHandshakeTimeout:   10 * time.Second,
		DisableKeepAlives:     true, // Disable keep-alive Ä‘á»ƒ force close connection
		MaxIdleConns:          0,    // KhÃ´ng giá»¯ idle connections
		MaxIdleConnsPerHost:   0,    // KhÃ´ng giá»¯ idle connections per host
	}

	transport := &RateLimitedTransport{
		baseTransport: baseTransport,
		rateLimiter:   rateLimiter,
		getAPIKey: func() (string, error) {
			keyInfo, err := keyPool.GetAvailableKey()
			if err != nil {
				return "", err
			}
			return keyInfo.Key, nil
		},
	}

	httpClient := &http.Client{
		Transport: transport,
		Timeout:   10 * time.Minute, // TÄƒng timeout lÃªn 10 phÃºt cho request lá»›n (50k bars)
	}

	return &Crawler{
		apiKey:      apiKeys[0], // Key Ä‘áº§u tiÃªn lÃ m default
		keyPool:     keyPool,
		rateLimiter: rateLimiter,
		httpClient:  httpClient,
		useKeyPool:  true,
	}, nil
}

// getAPIKey láº¥y API key Ä‘á»ƒ sá»­ dá»¥ng (tá»« pool náº¿u cÃ³, khÃ´ng thÃ¬ dÃ¹ng default)
func (c *Crawler) getAPIKey() (string, error) {
	if c.useKeyPool && c.keyPool != nil {
		keyInfo, err := c.keyPool.GetAvailableKey()
		if err != nil {
			return "", err
		}
		return keyInfo.Key, nil
	}
	return c.apiKey, nil
}

// GetKeyPoolStats tráº£ vá» thá»‘ng kÃª cá»§a key pool (náº¿u cÃ³)
func (c *Crawler) GetKeyPoolStats() map[string]interface{} {
	if c.useKeyPool && c.keyPool != nil {
		return c.keyPool.GetStats()
	}
	return nil
}

// Close Ä‘Ã³ng cÃ¡c káº¿t ná»‘i
func (c *Crawler) Close() error {
	if c.useKeyPool && c.keyPool != nil {
		return c.keyPool.Close()
	}
	if c.rateLimiter != nil {
		return c.rateLimiter.Close()
	}
	return nil
}

// Bar Ä‘áº¡i diá»‡n cho má»™t bar/candle trong dá»¯ liá»‡u aggregates
type Bar struct {
	Timestamp    int64   `json:"t" parquet:"t"`                      // Unix timestamp in milliseconds
	Open         float64 `json:"o" parquet:"o"`                      // Open
	High         float64 `json:"h" parquet:"h"`                      // High
	Low          float64 `json:"l" parquet:"l"`                      // Low
	Close        float64 `json:"c" parquet:"c"`                      // Close
	Volume       int64   `json:"v" parquet:"v"`                      // Volume
	VWAP         float64 `json:"vw,omitempty" parquet:"vw,optional"` // Volume weighted average price
	Transactions int64   `json:"n,omitempty" parquet:"n,optional"`   // Number of transactions
}

// splitDateRangeIntoChunks chia khoáº£ng thá»i gian [from, to] thÃ nh cÃ¡c chunk theo ngÃ y
// Ä‘á»ƒ Ä‘áº£m báº£o má»—i request khÃ´ng tráº£ vá» quÃ¡ ~maxLimit bars (vá»›i 1-minute bars).
func splitDateRangeIntoChunks(from, to time.Time, maxDays int) [][2]time.Time {
	var chunks [][2]time.Time

	// Chuáº©n hÃ³a vá» UTC Ä‘á»ƒ trÃ¡nh lá»‡ch do timezone
	start := from.UTC()
	end := to.UTC()

	if !start.Before(end) && !start.Equal(end) {
		return chunks
	}

	for currentStart := start; !currentStart.After(end); {
		// Má»—i chunk tá»‘i Ä‘a maxDays ngÃ y, inclusive
		currentEnd := currentStart.AddDate(0, 0, maxDays-1)
		if currentEnd.After(end) {
			currentEnd = end
		}

		chunks = append(chunks, [2]time.Time{currentStart, currentEnd})

		// Náº¿u Ä‘Ã£ tá»›i cuá»‘i khoáº£ng thá»i gian thÃ¬ dá»«ng
		if currentEnd.Equal(end) {
			break
		}

		// Chunk tiáº¿p theo báº¯t Ä‘áº§u tá»« ngÃ y káº¿ tiáº¿p
		currentStart = currentEnd.AddDate(0, 0, 1)
	}

	return chunks
}

// fetchWithNextURL gá»i API vá»›i URL vÃ  xá»­ lÃ½ next_url
// QUAN TRá»ŒNG: HÃ m nÃ y PHáº¢I Ä‘á»£i rate limiter trÆ°á»›c khi gá»i API
func (c *Crawler) fetchWithNextURL(requestURL string) (*AggregatesResponse, error) {
	// Láº¥y API key (náº¿u dÃ¹ng key pool, GetAvailableKey() Ä‘Ã£ Ä‘á»£i rate limiter rá»“i)
	apiKey, err := c.getAPIKey()
	if err != nil {
		return nil, fmt.Errorf("lá»—i khi láº¥y API key: %w", err)
	}

	// QUAN TRá»ŒNG: Äá»£i rate limiter TRÆ¯á»šC KHI gá»i API
	// Náº¿u dÃ¹ng key pool, GetAvailableKey() Ä‘Ã£ Ä‘á»£i rá»“i
	// NhÆ°ng náº¿u dÃ¹ng single key, pháº£i Ä‘á»£i á»Ÿ Ä‘Ã¢y
	if !c.useKeyPool {
		if err := c.rateLimiter.WaitForKey(apiKey); err != nil {
			return nil, fmt.Errorf("lá»—i khi Ä‘á»£i rate limiter: %w", err)
		}
	}
	// Náº¿u dÃ¹ng key pool, GetAvailableKey() Ä‘Ã£ gá»i WaitForKey rá»“i, khÃ´ng cáº§n Ä‘á»£i láº¡i

	// Äáº£m báº£o URL cÃ³ API key
	parsedURL, err := url.Parse(requestURL)
	if err != nil {
		return nil, fmt.Errorf("lá»—i khi parse URL: %w", err)
	}

	// ThÃªm API key náº¿u chÆ°a cÃ³ hoáº·c thay tháº¿ náº¿u Ä‘Ã£ cÃ³
	if parsedURL.Query().Get("apiKey") == "" {
		if parsedURL.RawQuery == "" {
			parsedURL.RawQuery = "apiKey=" + apiKey
		} else {
			parsedURL.RawQuery += "&apiKey=" + apiKey
		}
	} else {
		// Thay tháº¿ API key cÅ© báº±ng key má»›i tá»« pool
		query := parsedURL.Query()
		query.Set("apiKey", apiKey)
		parsedURL.RawQuery = query.Encode()
	}
	requestURL = parsedURL.String()

	log.Printf("Äang gá»i API: %s", requestURL)

	// Thá»±c hiá»‡n request vá»›i retry logic cho rate limit
	maxRetries := 3
	retryDelay := 15 * time.Second

	var resp *http.Response

	for attempt := 1; attempt <= maxRetries; attempt++ {
		var err error
		resp, err = c.httpClient.Get(requestURL)
		if err != nil {
			if attempt < maxRetries {
				log.Printf("Lá»—i network (attempt %d/%d), retry sau %v...", attempt, maxRetries, retryDelay)
				time.Sleep(retryDelay)
				continue
			}
			return nil, fmt.Errorf("lá»—i khi gá»i API sau %d attempts: %w", maxRetries, err)
		}

		// Kiá»ƒm tra status code
		if resp.StatusCode == http.StatusOK {
			break // ThÃ nh cÃ´ng
		}

		// Äá»c body Ä‘á»ƒ check error message
		body, _ := io.ReadAll(resp.Body)
		resp.Body.Close()

		// Náº¿u lÃ  rate limit (429), retry sau khi sleep
		if resp.StatusCode == http.StatusTooManyRequests || resp.StatusCode == 429 {
			if attempt < maxRetries {
				log.Printf("Rate limit exceeded (429) - attempt %d/%d, Ä‘á»£i %v trÆ°á»›c khi retry...", attempt, maxRetries, retryDelay)
				// Sleep 15 giÃ¢y Ä‘á»ƒ Ä‘á»£i rate limit reset
				time.Sleep(retryDelay)
				// Äá»£i thÃªm rate limiter trÆ°á»›c khi retry (khÃ´ng tÃ­nh request nÃ y vÃ o rate limit)
				if err := c.rateLimiter.WaitForKey(apiKey); err != nil {
					log.Printf("Cáº£nh bÃ¡o: khÃ´ng thá»ƒ Ä‘á»£i rate limiter: %v", err)
				}
				continue
			}
			return nil, fmt.Errorf("API tráº£ vá» rate limit (429) sau %d attempts: %s", maxRetries, string(body))
		}

		// CÃ¡c lá»—i khÃ¡c khÃ´ng retry
		return nil, fmt.Errorf("API tráº£ vá» status %d: %s", resp.StatusCode, string(body))
	}
	defer resp.Body.Close()

	// Parse JSON response
	var result AggregatesResponse
	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return nil, fmt.Errorf("lá»—i khi parse JSON: %w", err)
	}

	// Kiá»ƒm tra status trong response
	if result.Status != "OK" {
		return nil, fmt.Errorf("API tráº£ vá» status khÃ´ng OK: %s", result.Status)
	}

	return &result, nil
}

// CrawlMinuteBarsWithNextURL crawl dá»¯ liá»‡u minute bars gá»i API trá»±c tiáº¿p (khÃ´ng dÃ¹ng SDK).
// Bao gá»“m cáº£ extended hours (pre-market 4:00-9:30 ET vÃ  after-hours 16:00-20:00 ET).
// ÄÃƒ CHIA NHá» date range thÃ nh nhiá»u chunk theo ngÃ y Ä‘á»ƒ khÃ´ng vÆ°á»£t quÃ¡ limit 50k results/request.
func (c *Crawler) CrawlMinuteBarsWithNextURL(ticker string, from, to time.Time) ([]Bar, error) {
	var allBars []Bar

	// Chia khoáº£ng thá»i gian thÃ nh cÃ¡c chunk theo ngÃ y
	chunks := splitDateRangeIntoChunks(from, to, maxDaysPerRequest)
	if len(chunks) == 0 {
		log.Printf("[%s] KhÃ´ng cÃ³ chunk nÃ o trong khoáº£ng thá»i gian tá»« %s Ä‘áº¿n %s", ticker, from, to)
		return allBars, nil
	}

	log.Printf("[%s] Chia khoáº£ng thá»i gian thÃ nh %d chunks (tá»‘i Ä‘a %d ngÃ y/chunk)", ticker, len(chunks), maxDaysPerRequest)

	chunkCount := 0

	for chunkIndex, ch := range chunks {
		chunkFrom := ch[0]
		chunkTo := ch[1]

		// Náº¿u chunk cuá»‘i cÃ¹ng vÃ  chunkTo lÃ  ngÃ y hiá»‡n táº¡i, trá»« Ä‘i 1 ngÃ y Ä‘á»ƒ trÃ¡nh lá»—i DELAYED
		if chunkIndex == len(chunks)-1 {
			now := time.Now().UTC()
			today := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, time.UTC)
			chunkToDate := time.Date(chunkTo.Year(), chunkTo.Month(), chunkTo.Day(), 0, 0, 0, 0, time.UTC)
			if chunkToDate.Equal(today) || chunkToDate.After(today) {
				chunkTo = today.AddDate(0, 0, -1).Add(23*time.Hour + 59*time.Minute + 59*time.Second)
				log.Printf("[%s] Chunk cuá»‘i cÃ¹ng: Trá»« 1 ngÃ y Ä‘á»ƒ trÃ¡nh lá»—i DELAYED (tá»« %s)", ticker, chunkToDate.Format("2006-01-02"))
			}
		}

		fromStr := chunkFrom.Format("2006-01-02")
		toStr := chunkTo.Format("2006-01-02")

		log.Printf("[%s] Chunk %d/%d: Crawling tá»« %s Ä‘áº¿n %s", ticker, chunkIndex+1, len(chunks), fromStr, toStr)

		// Build URL vá»›i timestamp milliseconds
		fromMillis := chunkFrom.UnixMilli()
		toMillis := chunkTo.UnixMilli()

		// Gá»i API trá»±c tiáº¿p
		apiKey, err := c.getAPIKey()
		if err != nil {
			return nil, fmt.Errorf("lá»—i khi láº¥y API key: %w", err)
		}

		// Build URL: https://api.massive.com/v2/aggs/ticker/{ticker}/range/{multiplier}/{timespan}/{from}/{to}
		requestURL := fmt.Sprintf("https://api.massive.com/v2/aggs/ticker/%s/range/1/minute/%d/%d", ticker, fromMillis, toMillis)
		u, err := url.Parse(requestURL)
		if err != nil {
			return nil, fmt.Errorf("lá»—i khi parse URL: %w", err)
		}

		q := u.Query()
		q.Set("adjusted", "true")
		q.Set("limit", strconv.Itoa(maxLimit))
		q.Set("sort", "asc")
		q.Set("apiKey", apiKey)
		u.RawQuery = q.Encode()

		// Rate limiter Ä‘Ã£ Ä‘á»£i trong RoundTrip khi gá»i httpClient.Get
		// Gá»i API vá»›i retry logic
		var response *AggregatesResponse
		maxRetries := 3
		retryDelay := 15 * time.Second

		for attempt := 1; attempt <= maxRetries; attempt++ {
			req, err := http.NewRequest("GET", u.String(), nil)
			if err != nil {
				return nil, fmt.Errorf("lá»—i khi táº¡o request: %w", err)
			}
			req.Header.Set("Connection", "close")

			resp, err := c.httpClient.Do(req)
			if err != nil {
				if attempt < maxRetries {
					log.Printf("[%s] Chunk %d/%d - Lá»—i network (attempt %d/%d), retry sau %v...",
						ticker, chunkIndex+1, len(chunks), attempt, maxRetries, retryDelay)
					time.Sleep(retryDelay)
					continue
				}
				return nil, fmt.Errorf("lá»—i khi gá»i API sau %d attempts: %w", maxRetries, err)
			}

			// Kiá»ƒm tra status code
			if resp.StatusCode != http.StatusOK {
				body, _ := io.ReadAll(resp.Body)
				resp.Body.Close()

				if resp.StatusCode == http.StatusTooManyRequests || resp.StatusCode == 429 {
					if attempt < maxRetries {
						log.Printf("[%s] Chunk %d/%d - Rate limit (429) - attempt %d/%d, Ä‘á»£i %v trÆ°á»›c khi retry...",
							ticker, chunkIndex+1, len(chunks), attempt, maxRetries, retryDelay)
						time.Sleep(retryDelay)
						// Äá»£i thÃªm rate limiter trÆ°á»›c khi retry
						if err := c.rateLimiter.WaitForKey(apiKey); err != nil {
							log.Printf("Cáº£nh bÃ¡o: khÃ´ng thá»ƒ Ä‘á»£i rate limiter: %v", err)
						}
						continue
					}
					return nil, fmt.Errorf("API tráº£ vá» rate limit (429) sau %d attempts: %s", maxRetries, string(body))
				}

				resp.Body.Close()
				return nil, fmt.Errorf("API tráº£ vá» status %d: %s", resp.StatusCode, string(body))
			}

			// Parse JSON response
			var result AggregatesResponse
			if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
				resp.Body.Close()
				if attempt < maxRetries {
					log.Printf("[%s] Chunk %d/%d - Lá»—i parse JSON (attempt %d/%d), retry sau %v...",
						ticker, chunkIndex+1, len(chunks), attempt, maxRetries, retryDelay)
					time.Sleep(retryDelay)
					continue
				}
				return nil, fmt.Errorf("lá»—i khi parse JSON: %w", err)
			}
			resp.Body.Close()

			// Kiá»ƒm tra status trong response
			if result.Status != "OK" {
				// Náº¿u lÃ  DELAYED (dá»¯ liá»‡u chÆ°a sáºµn sÃ ng), skip chunk nÃ y vÃ  tiáº¿p tá»¥c
				if result.Status == "DELAYED" {
					log.Printf("[%s] Chunk %d/%d - Dá»¯ liá»‡u DELAYED (chÆ°a sáºµn sÃ ng), skip chunk nÃ y",
						ticker, chunkIndex+1, len(chunks))
					response = nil // ÄÃ¡nh dáº¥u lÃ  Ä‘Ã£ skip
					break
				}
				return nil, fmt.Errorf("API tráº£ vá» status khÃ´ng OK: %s", result.Status)
			}

			response = &result
			break // ThÃ nh cÃ´ng
		}

		// Náº¿u response lÃ  nil (do DELAYED hoáº·c lá»—i), skip chunk nÃ y
		if response == nil {
			// DELAYED Ä‘Ã£ Ä‘Æ°á»£c log á»Ÿ trÃªn, skip chunk nÃ y vÃ  tiáº¿p tá»¥c
			log.Printf("[%s] Chunk %d/%d - Skip do DELAYED hoáº·c lá»—i", ticker, chunkIndex+1, len(chunks))
			continue
		}

		// Convert BarRaw sang Bar
		chunkBars := 0
		for _, barRaw := range response.Results {
			allBars = append(allBars, barRaw.ToBar())
			chunkBars++
		}

		// Náº¿u cÃ³ next_url, cÃ³ nghÄ©a lÃ  chunk nÃ y cÃ³ nhiá»u hÆ¡n 50k bars
		if response.NextURL != "" {
			log.Printf("[%s] Chunk %d/%d - Cáº¢NH BÃO: CÃ³ next_url (cÃ³ thá»ƒ vÆ°á»£t quÃ¡ 50k bars)",
				ticker, chunkIndex+1, len(chunks))
		}

		chunkCount++
		log.Printf("[%s] Chunk %d/%d - HoÃ n thÃ nh: %d bars (tá»•ng: %d)",
			ticker, chunkIndex+1, len(chunks), chunkBars, len(allBars))
	}

	log.Printf("[%s] HoÃ n thÃ nh: %d chunks, tá»•ng %d bars", ticker, chunkCount, len(allBars))

	// LÆ°u 1 file duy nháº¥t cho cáº£ ticker (toÃ n bá»™ khoáº£ng fromâ€“to) náº¿u SavePacketDir vÃ  PacketSaver Ä‘Æ°á»£c set
	if c.SavePacketDir != "" && c.PacketSaver != nil && len(allBars) > 0 {
		tickerDir := filepath.Join(c.SavePacketDir, ticker)
		if err := os.MkdirAll(tickerDir, 0755); err != nil {
			log.Printf("[%s] âš ï¸ Save: khÃ´ng táº¡o Ä‘Æ°á»£c folder %s: %v", ticker, tickerDir, err)
		} else {
			ext := c.PacketSaver.Extension()
			packetName := fmt.Sprintf("%s_%s_to_%s.%s",
				ticker, from.Format("2006-01-02"), to.Format("2006-01-02"), ext)
			packetPath := filepath.Join(tickerDir, packetName)
			saverBars := make([]saver.Bar, len(allBars))
			for i, b := range allBars {
				saverBars[i] = saver.Bar{
					Timestamp: b.Timestamp, Open: b.Open, High: b.High, Low: b.Low,
					Close: b.Close, Volume: b.Volume, VWAP: b.VWAP, Transactions: b.Transactions,
				}
			}
			if err := c.PacketSaver.Save(saverBars, packetPath); err != nil {
				log.Printf("[%s] âš ï¸ Save: lá»—i lÆ°u %s: %v", ticker, packetPath, err)
			} else {
				log.Printf("[%s] ğŸ“¦ Saved 1 file (%s): %s (%d bars)", ticker, ext, packetPath, len(allBars))
			}
		}
	}

	return allBars, nil
}

// CrawlMinuteBars crawl dá»¯ liá»‡u minute bars (wrapper Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch)
func (c *Crawler) CrawlMinuteBars(ticker string, from, to time.Time) ([]Bar, error) {
	return c.CrawlMinuteBarsWithNextURL(ticker, from, to)
}
